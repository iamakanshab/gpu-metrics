apiVersion: v1
kind: Namespace
metadata:
  name: gpu-metrics-test
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: gpu-metrics-collector
  namespace: gpu-metrics-test
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: gpu-metrics-reader
rules:
- apiGroups: [""]
  resources: ["pods", "nodes"]
  verbs: ["get", "list", "watch"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: gpu-metrics-reader
subjects:
- kind: ServiceAccount
  name: gpu-metrics-collector
  namespace: gpu-metrics-test
roleRef:
  kind: ClusterRole
  name: gpu-metrics-reader
  apiGroup: rbac.authorization.k8s.io
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: gpu-collector-script
  namespace: gpu-metrics-test
data:
  collector.py: |
    #!/usr/bin/env python3
    import logging
    from prometheus_client import start_http_server, Gauge, Counter
    import time
    import os
    from kubernetes import client, config
    import subprocess
    import sys
    import traceback
    import re
    from datetime import datetime

    # Configure logging
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    logger = logging.getLogger('k8s-gpu-exporter')

    class K8sGPUMetrics:
        def __init__(self):
            logger.info("Initializing metrics with namespace support...")
            
            # GPU metrics with namespace label
            self.gpu_utilization = Gauge(
                'k8s_gpu_utilization', 
                'GPU utilization percentage',
                ['node', 'gpu_id', 'namespace', 'pod']
            )
            self.gpu_memory = Gauge(
                'k8s_gpu_memory',
                'GPU memory usage percentage',
                ['node', 'gpu_id', 'namespace', 'pod']
            )
            self.gpu_power = Gauge(
                'k8s_gpu_power',
                'GPU power usage in watts',
                ['node', 'gpu_id', 'namespace', 'pod']
            )
            
            # Collection status metrics
            self.last_collection = Gauge(
                'k8s_gpu_last_collection_timestamp',
                'Timestamp of last successful collection'
            )
            self.collection_errors = Counter(
                'k8s_gpu_collector_errors_total',
                'Total number of collection errors',
                ['type']
            )

    def parse_gpu_metrics(output):
        """Parse rocm-smi output into structured data."""
        metrics = {}
        current_gpu = None
        
        for line in output.split('\n'):
            line = line.strip()
            
            # Match GPU index
            gpu_match = re.match(r'GPU\[(\d+)\]', line)
            if gpu_match:
                current_gpu = gpu_match.group(1)
                if current_gpu not in metrics:
                    metrics[current_gpu] = {}
                
            # Parse different metrics
            if current_gpu is not None:
                if 'Current Socket Graphics Package Power (W):' in line:
                    try:
                        power = float(line.split(':')[1].strip())
                        metrics[current_gpu]['power'] = power
                    except (ValueError, IndexError) as e:
                        logger.error(f"Error parsing power value: {e}")
                    
                elif 'GPU use (%):' in line:
                    try:
                        utilization = float(line.split(':')[1].strip())
                        metrics[current_gpu]['utilization'] = utilization
                    except (ValueError, IndexError) as e:
                        logger.error(f"Error parsing utilization value: {e}")
                    
                elif 'GPU Memory Allocated (VRAM%):' in line:
                    try:
                        memory = float(line.split(':')[1].strip())
                        metrics[current_gpu]['memory'] = memory
                    except (ValueError, IndexError) as e:
                        logger.error(f"Error parsing memory value: {e}")
                    
        return metrics

    def get_gpu_pod_mapping():
        """Get mapping of GPUs to pods and namespaces."""
        try:
            config.load_incluster_config()
            v1 = client.CoreV1Api()
            
            current_node = os.uname().nodename
            pods = v1.list_pod_for_all_namespaces(
                field_selector=f'spec.nodeName={current_node}'
            )
            
            gpu_mapping = {}
            for pod in pods.items:
                # Check for GPU usage in the pod
                for container in pod.spec.containers:
                    if container.resources and container.resources.limits:
                        for resource_name in container.resources.limits:
                            if 'amd.com/gpu' in resource_name:
                                # For simplicity, assume GPU IDs match container order
                                # You might need to adjust this based on your setup
                                gpu_mapping[str(len(gpu_mapping))] = {
                                    'namespace': pod.metadata.namespace,
                                    'pod': pod.metadata.name
                                }
            
            return gpu_mapping
        except Exception as e:
            logger.error(f"Error getting GPU pod mapping: {e}")
            return {}

    def get_gpu_metrics():
        """Get GPU metrics using rocm-smi with detailed logging."""
        logger.info("Attempting to collect GPU metrics...")
        try:
            # Get GPU metrics
            result = subprocess.run(
                ['rocm-smi', '--showuse', '--showmemuse', '--showpower'],
                capture_output=True,
                text=True,
                timeout=30
            )
            
            if result.returncode != 0:
                logger.error(f"rocm-smi error: {result.stderr}")
                return None
                
            logger.info(f"GPU metrics collected successfully")
            return parse_gpu_metrics(result.stdout)

        except subprocess.TimeoutExpired:
            logger.error("rocm-smi command timed out")
            return None
        except Exception as e:
            logger.error(f"Error collecting GPU metrics: {str(e)}")
            return None

    class K8sGPUExporter:
        def __init__(self):
            logger.info("Initializing exporter...")
            self.metrics = K8sGPUMetrics()
            self.current_node = os.uname().nodename
            logger.info(f"Running on node: {self.current_node}")

        def update_metrics(self, gpu_metrics):
            """Update Prometheus metrics with GPU data."""
            try:
                # Get GPU to pod mapping
                gpu_mapping = get_gpu_pod_mapping()
                
                for gpu_id, metrics in gpu_metrics.items():
                    namespace = 'unmapped'
                    pod_name = 'unmapped'
                    
                    if gpu_id in gpu_mapping:
                        namespace = gpu_mapping[gpu_id]['namespace']
                        pod_name = gpu_mapping[gpu_id]['pod']
                    
                    # Update utilization
                    if 'utilization' in metrics:
                        self.metrics.gpu_utilization.labels(
                            node=self.current_node,
                            gpu_id=gpu_id,
                            namespace=namespace,
                            pod=pod_name
                        ).set(metrics['utilization'])
                    
                    # Update memory
                    if 'memory' in metrics:
                        self.metrics.gpu_memory.labels(
                            node=self.current_node,
                            gpu_id=gpu_id,
                            namespace=namespace,
                            pod=pod_name
                        ).set(metrics['memory'])
                    
                    # Update power
                    if 'power' in metrics:
                        self.metrics.gpu_power.labels(
                            node=self.current_node,
                            gpu_id=gpu_id,
                            namespace=namespace,
                            pod=pod_name
                        ).set(metrics['power'])
                        
                    logger.info(f"Updated metrics for GPU {gpu_id}: {metrics}")
                    
                # Update last collection timestamp
                self.metrics.last_collection.set_to_current_time()
                    
            except Exception as e:
                logger.error(f"Error updating metrics: {e}")
                self.metrics.collection_errors.labels(type='update_metrics').inc()

        def collect_metrics(self):
            """Collect and update GPU metrics."""
            try:
                logger.info("Starting metrics collection...")
                
                # Get GPU metrics
                gpu_metrics = get_gpu_metrics()
                if not gpu_metrics:
                    logger.error("Failed to collect GPU metrics")
                    return

                # Update Prometheus metrics
                self.update_metrics(gpu_metrics)
                logger.info("Metrics collection completed successfully")

            except Exception as e:
                logger.error(f"Error in collect_metrics: {e}")
                logger.error(traceback.format_exc())

    def main():
        try:
            port = int(os.environ.get('EXPORTER_PORT', 9400))
            collection_interval = int(os.environ.get('COLLECTION_INTERVAL', 30))

            logger.info(f"Starting Kubernetes GPU exporter on port {port}")
            logger.info(f"Collection interval: {collection_interval} seconds")
            
            # Start Prometheus HTTP server
            start_http_server(port)
            
            exporter = K8sGPUExporter()
            
            while True:
                try:
                    exporter.collect_metrics()
                    time.sleep(collection_interval)
                except Exception as e:
                    logger.error(f"Error in collection loop: {e}")
                    time.sleep(collection_interval)
                    
        except Exception as e:
            logger.error(f"Fatal error in main: {e}")
            logger.error(traceback.format_exc())
            sys.exit(1)

    if __name__ == "__main__":
        main()
---
apiVersion: v1
kind: Pod
metadata:
  name: gpu-metrics-collector-test
  namespace: gpu-metrics-test
  labels:
    app: gpu-metrics-collector
spec:
  serviceAccountName: gpu-metrics-collector
  containers:
  - name: gpu-collector
    image: python:3.9-slim
    command:
    - /bin/bash
    - -c
    - |
      apt-get update && \
      apt-get install -y python3-pip curl procps && \
      pip install kubernetes prometheus-client && \
      python3 /app/collector.py
    ports:
    - containerPort: 9400
      name: metrics
    env:
    - name: COLLECTION_INTERVAL
      value: "30"
    - name: EXPORTER_PORT
      value: "9400"
    volumeMounts:
    - name: collector-script
      mountPath: /app
    - name: rocm
      mountPath: /opt/rocm
    securityContext:
      privileged: true
    resources:
      requests:
        memory: "256Mi"
        cpu: "100m"
      limits:
        memory: "512Mi"
        cpu: "200m"
  volumes:
  - name: collector-script
    configMap:
      name: gpu-collector-script
      defaultMode: 0777
  - name: rocm
    hostPath:
      path: /opt/rocm
---
apiVersion: v1
kind: Service
metadata:
  name: gpu-metrics-collector-test
  namespace: gpu-metrics-test
spec:
  selector:
    app: gpu-metrics-collector
  ports:
  - port: 9400
    targetPort: metrics
    protocol: TCP
    name: metrics
